

- Random integer:  11 

- Dummy sample before slicing is : torch.Size([1, 28, 28]) 

- Dummy sample after slicing is : torch.Size([1, 4, 28]) 

- sample shape after squeezing is : torch.Size([1, 1, 4, 28]) 

----------- Convolution Size Variation through layers       -------------- 

--> Initial sizes are: 
 width =  28 and height =  4 

--> After Conv layer # 0 
 width =  26 and height =  2 

--> After pooling layer # 0 
 width =  13 and height =  1 

--> After Conv layer # 1 
 width =  11 and height =  -1 

--> After pooling layer # 1 
 width =  5 and height =  -1 

--> After Conv layer # 2 
 width =  3 and height =  -3 

--> After pooling layer # 2 
 width =  1 and height =  -2 

--> After Conv layer # 3 
 width =  -1 and height =  -4 

--> After pooling layer # 3 
 width =  -1 and height =  -2 

----------- Encoder Architecture -------------- 

Printing the CNN Model layers name and their corresponding              shape: 

CNN_part.0.0.weight 	 	 torch.Size([64, 1, 3, 3]) 

CNN_part.0.0.bias 	 	 torch.Size([64]) 

CNN_part.0.1.weight 	 	 torch.Size([64]) 

CNN_part.0.1.bias 	 	 torch.Size([64]) 

CNN_part.1.0.weight 	 	 torch.Size([128, 64, 3, 3]) 

CNN_part.1.0.bias 	 	 torch.Size([128]) 

CNN_part.1.1.weight 	 	 torch.Size([128]) 

CNN_part.1.1.bias 	 	 torch.Size([128]) 

CNN_part.2.0.weight 	 	 torch.Size([256, 128, 3, 3]) 

CNN_part.2.0.bias 	 	 torch.Size([256]) 

CNN_part.2.1.weight 	 	 torch.Size([256]) 

CNN_part.2.1.bias 	 	 torch.Size([256]) 

CNN_part.3.0.weight 	 	 torch.Size([256, 256, 3, 3]) 

CNN_part.3.0.bias 	 	 torch.Size([256]) 

CNN_part.3.1.weight 	 	 torch.Size([256]) 

CNN_part.3.1.bias 	 	 torch.Size([256]) 

dense_part.0.0.weight 	 	 torch.Size([128, 512]) 

dense_part.0.0.bias 	 	 torch.Size([128]) 

---------------------------- 

-- > Testing forward propagation through encoder part 

RuntimeError: Calculated padded input size per channel: (1 x 13). Kernel size: (3 x 3). 
Kernel size can't be greater than actual input size


